{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: biopython in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (1.83)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from biopython) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install biopython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import numpy as np\n",
    "import seaborn as sbn\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PMID  OWN                STAT        LR  \\\n",
      "0  38962644  NLM  PubMed-not-MEDLINE  20240704   \n",
      "1  38961098  NLM             MEDLINE  20240703   \n",
      "2  38960095  NLM           Publisher  20240703   \n",
      "3  38959985  NLM           Publisher  20240703   \n",
      "4  38959725  NLM           Publisher  20240703   \n",
      "\n",
      "                                                  IS   VI   IP           DP  \\\n",
      "0  2168-8184 (Print) 2168-8184 (Electronic) 2168-...   16    6     2024 Jun   \n",
      "1         2045-2322 (Electronic) 2045-2322 (Linking)   14    1   2024 Jul 3   \n",
      "2         1615-5947 (Electronic) 0890-5096 (Linking)  NaN  NaN   2024 Jul 1   \n",
      "3         1532-8406 (Electronic) 0883-5403 (Linking)  NaN  NaN   2024 Jul 1   \n",
      "4         1873-460X (Electronic) 1056-8727 (Linking)   38    8  2024 Jun 30   \n",
      "\n",
      "                                                  TI      PG  ...  \\\n",
      "0  Association of Different Anticoagulation Strat...  e61545  ...   \n",
      "1  A collaborative pharmacist-led intervention to...   15285  ...   \n",
      "2  Effect of Chronic Steroid Use on Postoperative...     NaN  ...   \n",
      "3  Robot-Assisted Total Hip Arthroplasty is Assoc...     NaN  ...   \n",
      "4  Risk factors at admission of in-hospital dysgl...  108803  ...   \n",
      "\n",
      "                                                  SO      DCOM   SB  \\\n",
      "0  Cureus. 2024 Jun 2;16(6):e61545. doi: 10.7759/...       NaN  NaN   \n",
      "1  Sci Rep. 2024 Jul 3;14(1):15285. doi: 10.1038/...  20240703   IM   \n",
      "2  Ann Vasc Surg. 2024 Jul 1:S0890-5096(24)00296-...       NaN   IM   \n",
      "3  J Arthroplasty. 2024 Jul 1:S0883-5403(24)00649...       NaN   IM   \n",
      "4  J Diabetes Complications. 2024 Jun 30;38(8):10...       NaN   IM   \n",
      "\n",
      "                                                  MH AUID   TT   RN   CN   GR  \\\n",
      "0                                                NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1  [Humans, *Emergency Service, Hospital, *Patien...  NaN  NaN  NaN  NaN  NaN   \n",
      "2                                                NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "3                                                NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "4                                                NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "    SI  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "Entrez.email = \"mjyothionline@gmail.com\"  # Set your email\n",
    "\n",
    "def fetch_pubmed_data(query, max_results=100):\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    id_list = record[\"IdList\"]\n",
    "    \n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"medline\", retmode=\"text\")\n",
    "    records = handle.read()\n",
    "    handle.close()\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Example query\n",
    "query = \"patient readmission\"\n",
    "data = fetch_pubmed_data(query, max_results=50)\n",
    "\n",
    "# Parse and display the data\n",
    "from Bio import Medline\n",
    "records = list(Medline.parse(data.splitlines()))\n",
    "df = pd.DataFrame(records)\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    cleaned_abstract\n",
      "0  Background Therapeutic anticoagulation is the ...\n",
      "1  Unplanned hospital readmission is a safety and...\n",
      "2  OBJECTIVE: While existing literature reports a...\n",
      "3  INTRODUCTION: Total Hip Arthroplasty (THA) aim...\n",
      "4  AIMS: In-hospital dysglycemia is associated wi...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_abstract'] = df['AB'].apply(lambda x: clean_text(str(x)))\n",
    "print(df[['cleaned_abstract']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pubmed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Association of Different Anticoagulation Strat...   \n",
      "1  A collaborative pharmacist-led intervention to...   \n",
      "2  Effect of Chronic Steroid Use on Postoperative...   \n",
      "3  Robot-Assisted Total Hip Arthroplasty is Assoc...   \n",
      "4  Risk factors at admission of in-hospital dysgl...   \n",
      "\n",
      "                                             Authors  \\\n",
      "0  ['Rehman A', 'Bahk J', 'Baloch HNU', 'Salman S...   \n",
      "1                       ['Tran-Nguyen S', 'Asha SE']   \n",
      "2  ['Chamseddine H', 'Sawma T', 'Slika H', 'Panos...   \n",
      "3  ['Singh A', 'Kotzur T', 'Peng L', 'Emukah C', ...   \n",
      "4  ['Olsen MT', 'Klarskov CK', 'Hansen KB', 'Pede...   \n",
      "\n",
      "                                            Abstract  \n",
      "0  Background Therapeutic anticoagulation is the ...  \n",
      "1  Unplanned hospital readmission is a safety and...  \n",
      "2  OBJECTIVE: While existing literature reports a...  \n",
      "3  INTRODUCTION: Total Hip Arthroplasty (THA) aim...  \n",
      "4  AIMS: In-hospital dysglycemia is associated wi...  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Association of Different Anticoagulation Strat...</td>\n",
       "      <td>['Rehman A', 'Bahk J', 'Baloch HNU', 'Salman S...</td>\n",
       "      <td>Background Therapeutic anticoagulation is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A collaborative pharmacist-led intervention to...</td>\n",
       "      <td>['Tran-Nguyen S', 'Asha SE']</td>\n",
       "      <td>Unplanned hospital readmission is a safety and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Effect of Chronic Steroid Use on Postoperative...</td>\n",
       "      <td>['Chamseddine H', 'Sawma T', 'Slika H', 'Panos...</td>\n",
       "      <td>OBJECTIVE: While existing literature reports a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robot-Assisted Total Hip Arthroplasty is Assoc...</td>\n",
       "      <td>['Singh A', 'Kotzur T', 'Peng L', 'Emukah C', ...</td>\n",
       "      <td>INTRODUCTION: Total Hip Arthroplasty (THA) aim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Risk factors at admission of in-hospital dysgl...</td>\n",
       "      <td>['Olsen MT', 'Klarskov CK', 'Hansen KB', 'Pede...</td>\n",
       "      <td>AIMS: In-hospital dysglycemia is associated wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Association of Different Anticoagulation Strat...   \n",
       "1  A collaborative pharmacist-led intervention to...   \n",
       "2  Effect of Chronic Steroid Use on Postoperative...   \n",
       "3  Robot-Assisted Total Hip Arthroplasty is Assoc...   \n",
       "4  Risk factors at admission of in-hospital dysgl...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  ['Rehman A', 'Bahk J', 'Baloch HNU', 'Salman S...   \n",
       "1                       ['Tran-Nguyen S', 'Asha SE']   \n",
       "2  ['Chamseddine H', 'Sawma T', 'Slika H', 'Panos...   \n",
       "3  ['Singh A', 'Kotzur T', 'Peng L', 'Emukah C', ...   \n",
       "4  ['Olsen MT', 'Klarskov CK', 'Hansen KB', 'Pede...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  Background Therapeutic anticoagulation is the ...  \n",
       "1  Unplanned hospital readmission is a safety and...  \n",
       "2  OBJECTIVE: While existing literature reports a...  \n",
       "3  INTRODUCTION: Total Hip Arthroplasty (THA) aim...  \n",
       "4  AIMS: In-hospital dysglycemia is associated wi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('pubmed_data.csv')\n",
    "\n",
    "# Rename the columns for better readability\n",
    "df.rename(columns={'TI': 'Title', 'AU': 'Authors', 'AB': 'Abstract'}, inplace=True)\n",
    "\n",
    "# Display the first 5 rows of the renamed columns\n",
    "display_columns = ['Title', 'Authors', 'Abstract']\n",
    "print(df[display_columns].head())\n",
    "\n",
    "# Display the DataFrame in a more readable format in Jupyter Notebook\n",
    "from IPython.display import display\n",
    "display(df[display_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Association of Different Anticoagulation Strat...</td>\n",
       "      <td>['Rehman A', 'Bahk J', 'Baloch HNU', 'Salman S...</td>\n",
       "      <td>Background Therapeutic anticoagulation is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A collaborative pharmacist-led intervention to...</td>\n",
       "      <td>['Tran-Nguyen S', 'Asha SE']</td>\n",
       "      <td>Unplanned hospital readmission is a safety and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Effect of Chronic Steroid Use on Postoperative...</td>\n",
       "      <td>['Chamseddine H', 'Sawma T', 'Slika H', 'Panos...</td>\n",
       "      <td>OBJECTIVE: While existing literature reports a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robot-Assisted Total Hip Arthroplasty is Assoc...</td>\n",
       "      <td>['Singh A', 'Kotzur T', 'Peng L', 'Emukah C', ...</td>\n",
       "      <td>INTRODUCTION: Total Hip Arthroplasty (THA) aim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Risk factors at admission of in-hospital dysgl...</td>\n",
       "      <td>['Olsen MT', 'Klarskov CK', 'Hansen KB', 'Pede...</td>\n",
       "      <td>AIMS: In-hospital dysglycemia is associated wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Association of Different Anticoagulation Strat...   \n",
       "1  A collaborative pharmacist-led intervention to...   \n",
       "2  Effect of Chronic Steroid Use on Postoperative...   \n",
       "3  Robot-Assisted Total Hip Arthroplasty is Assoc...   \n",
       "4  Risk factors at admission of in-hospital dysgl...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  ['Rehman A', 'Bahk J', 'Baloch HNU', 'Salman S...   \n",
       "1                       ['Tran-Nguyen S', 'Asha SE']   \n",
       "2  ['Chamseddine H', 'Sawma T', 'Slika H', 'Panos...   \n",
       "3  ['Singh A', 'Kotzur T', 'Peng L', 'Emukah C', ...   \n",
       "4  ['Olsen MT', 'Klarskov CK', 'Hansen KB', 'Pede...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  Background Therapeutic anticoagulation is the ...  \n",
       "1  Unplanned hospital readmission is a safety and...  \n",
       "2  OBJECTIVE: While existing literature reports a...  \n",
       "3  INTRODUCTION: Total Hip Arthroplasty (THA) aim...  \n",
       "4  AIMS: In-hospital dysglycemia is associated wi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('pubmed_data.csv')\n",
    "\n",
    "# Rename the columns for better readability\n",
    "df.rename(columns={'TI': 'Title', 'AU': 'Authors', 'AB': 'Abstract'}, inplace=True)\n",
    "\n",
    "# Display the first 5 rows of the renamed columns\n",
    "display_columns = ['Title', 'Authors', 'Abstract']\n",
    "\n",
    "# Display the DataFrame in a more readable format in Jupyter Notebook\n",
    "from IPython.display import display\n",
    "display(df[display_columns].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/zeeshanhaider41/mimic-iii-homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all files in the input directory\n",
    "input_dir = \"\"\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for filename in files:\n",
    "        print(os.path.join(root, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Correct path to the ZIP file\n",
    "zip_file_path = \"MIMIC -III_10000patients.zip\"\n",
    "extraction_dir = \"mimic-iii-10k\"\n",
    "\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extraction_dir, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic-iii-10k\\ADMISSIONS\\ADMISSIONS_random.csv\n",
      "mimic-iii-10k\\ADMISSIONS\\ADMISSIONS_sorted.csv\n",
      "mimic-iii-10k\\CALLOUT\\CALLOUT_random.csv\n",
      "mimic-iii-10k\\CALLOUT\\CALLOUT_sorted.csv\n",
      "mimic-iii-10k\\CAREGIVERS\\CAREGIVERS.csv\n",
      "mimic-iii-10k\\CPTEVENTS\\CPTEVENTS_random.csv\n",
      "mimic-iii-10k\\CPTEVENTS\\CPTEVENTS_sorted.csv\n",
      "mimic-iii-10k\\DATETIMEEVENTS\\DATETIMEEVENTS_CV_random.csv\n",
      "mimic-iii-10k\\DATETIMEEVENTS\\DATETIMEEVENTS_CV_sorted.csv\n",
      "mimic-iii-10k\\DIAGNOSES_ICD\\DIAGNOSES_ICD_random.csv\n",
      "mimic-iii-10k\\DIAGNOSES_ICD\\DIAGNOSES_ICD_sorted.csv\n",
      "mimic-iii-10k\\DRGCODES\\DRGCODES_random.csv\n",
      "mimic-iii-10k\\DRGCODES\\DRGCODES_sorted.csv\n",
      "mimic-iii-10k\\D_CPT\\D_CPT.csv\n",
      "mimic-iii-10k\\D_ICD_DIAGNOSES\\D_ICD_DIAGNOSES.csv\n",
      "mimic-iii-10k\\D_ICD_PROCEDURES\\D_ICD_PROCEDURES.csv\n",
      "mimic-iii-10k\\D_ITEMS\\D_ITEMS.csv\n",
      "mimic-iii-10k\\D_LABITEMS\\D_LABITEMS.csv\n",
      "mimic-iii-10k\\ICUSTAYS\\ICUSTAYS_random.csv\n",
      "mimic-iii-10k\\ICUSTAYS\\ICUSTAYS_sorted.csv\n",
      "mimic-iii-10k\\INPUTEVENTS_CV\\INPUTEVENTS_CV_random.csv\n",
      "mimic-iii-10k\\INPUTEVENTS_CV\\INPUTEVENTS_CV_sorted.csv\n",
      "mimic-iii-10k\\INPUTEVENTS_MV\\INPUTEVENTS_MV_random.csv\n",
      "mimic-iii-10k\\INPUTEVENTS_MV\\INPUTEVENTS_MV_sorted.csv\n",
      "mimic-iii-10k\\LABEVENTS\\LABEVENTS_random.csv\n",
      "mimic-iii-10k\\LABEVENTS\\LABEVENTS_sorted.csv\n",
      "mimic-iii-10k\\MICROBIOLOGYEVENTS\\MICROBIOLOGYEVENTS_random.csv\n",
      "mimic-iii-10k\\MICROBIOLOGYEVENTS\\MICROBIOLOGYEVENTS_sorted.csv\n",
      "mimic-iii-10k\\NOTEEVENTS\\NOTEEVENTS_random.csv\n",
      "mimic-iii-10k\\NOTEEVENTS\\NOTEEVENTS_sorted.csv\n",
      "mimic-iii-10k\\OUTPUTEVENTS\\OUTPUTEVENTS_random.csv\n",
      "mimic-iii-10k\\OUTPUTEVENTS\\OUTPUTEVENTS_sorted.csv\n",
      "mimic-iii-10k\\PATIENTS\\PATIENTS_random.csv\n",
      "mimic-iii-10k\\PATIENTS\\PATIENTS_sorted.csv\n",
      "mimic-iii-10k\\PRESCRIPTIONS\\PRESCRIPTIONS_random.csv\n",
      "mimic-iii-10k\\PRESCRIPTIONS\\PRESCRIPTIONS_sorted.csv\n",
      "mimic-iii-10k\\PROCEDUREEVENTS_MV\\PROCEDUREEVENTS_MV_random.csv\n",
      "mimic-iii-10k\\PROCEDUREEVENTS_MV\\PROCEDUREEVENTS_MV_sorted.csv\n",
      "mimic-iii-10k\\PROCEDURES_ICD\\PROCEDURES_ICD_random.csv\n",
      "mimic-iii-10k\\PROCEDURES_ICD\\PROCEDURES_ICD_sorted.csv\n",
      "mimic-iii-10k\\SERVICES\\SERVICES_random.csv\n",
      "mimic-iii-10k\\SERVICES\\SERVICES_sorted.csv\n",
      "mimic-iii-10k\\TRANSFERS\\TRANSFERS_random.csv\n",
      "mimic-iii-10k\\TRANSFERS\\TRANSFERS_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "# List all files in the extraction directory\n",
    "for root, dirs, files in os.walk(extraction_dir):\n",
    "    for filename in files:\n",
    "        print(os.path.join(root, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1678764</td>\n",
       "      <td>2</td>\n",
       "      <td>163353.0</td>\n",
       "      <td>2138-07-17</td>\n",
       "      <td>2138-07-17 22:51:00</td>\n",
       "      <td>2138-07-17 23:12:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16929.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology Attending Triage Note\\n\\nBaby [**N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1678765</td>\n",
       "      <td>2</td>\n",
       "      <td>163353.0</td>\n",
       "      <td>2138-07-17</td>\n",
       "      <td>2138-07-17 23:08:00</td>\n",
       "      <td>2138-07-17 23:18:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17774.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nursing Transfer note\\n\\n\\nPt admitted to NICU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272794</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2101-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nInferior/lateral ST-T changes ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769224</td>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>2101-10-26</td>\n",
       "      <td>2101-10-26 06:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272793</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2101-10-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nA-V delay\\nNonspecific inferior ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "0  1678764           2  163353.0  2138-07-17  2138-07-17 22:51:00   \n",
       "1  1678765           2  163353.0  2138-07-17  2138-07-17 23:08:00   \n",
       "2   272794           3       NaN  2101-10-06                  NaN   \n",
       "3   769224           3  145834.0  2101-10-26  2101-10-26 06:01:00   \n",
       "4   272793           3       NaN  2101-10-11                  NaN   \n",
       "\n",
       "             STORETIME       CATEGORY          DESCRIPTION     CGID  ISERROR  \\\n",
       "0  2138-07-17 23:12:00  Nursing/other               Report  16929.0      NaN   \n",
       "1  2138-07-17 23:18:00  Nursing/other               Report  17774.0      NaN   \n",
       "2                  NaN            ECG               Report      NaN      NaN   \n",
       "3                  NaN      Radiology  CHEST (PORTABLE AP)      NaN      NaN   \n",
       "4                  NaN            ECG               Report      NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Neonatology Attending Triage Note\\n\\nBaby [**N...  \n",
       "1  Nursing Transfer note\\n\\n\\nPt admitted to NICU...  \n",
       "2  Sinus rhythm\\nInferior/lateral ST-T changes ar...  \n",
       "3  [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...  \n",
       "4  Sinus rhythm\\nA-V delay\\nNonspecific inferior ...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the correct paths to the CSV files\n",
    "noteevents_path = os.path.join(\"mimic-iii-10k/NOTEEVENTS/NOTEEVENTS_sorted.csv\")\n",
    "admissions_path = os.path.join(\"mimic-iii-10k/ADMISSIONS/ADMISSIONS_sorted.csv\")\n",
    "\n",
    "# Load the relevant CSV files into DataFrames\n",
    "noteevents = pd.read_csv(noteevents_path)\n",
    "admissions = pd.read_csv(admissions_path)\n",
    "\n",
    "# Display the first few rows of the DataFrames\n",
    "noteevents.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>DISCHARGE_LOCATION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>RELIGION</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>EDREGTIME</th>\n",
       "      <th>EDOUTTIME</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>HAS_CHARTEVENTS_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-17 19:04:00</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>2101-10-20 19:08:00</td>\n",
       "      <td>2101-10-31 13:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>SNF</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2101-10-20 17:09:00</td>\n",
       "      <td>2101-10-20 19:24:00</td>\n",
       "      <td>HYPOTENSION</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>2191-03-16 00:28:00</td>\n",
       "      <td>2191-03-23 18:41:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>HOME WITH HOME IV PROVIDR</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROTESTANT QUAKER</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2191-03-15 13:10:00</td>\n",
       "      <td>2191-03-16 01:10:00</td>\n",
       "      <td>FEVER,DEHYDRATION,FAILURE TO THRIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>178980</td>\n",
       "      <td>2103-02-02 04:31:00</td>\n",
       "      <td>2103-02-04 12:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUDDHIST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>2175-05-30 07:15:00</td>\n",
       "      <td>2175-06-15 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHRONIC RENAL FAILURE/SDA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID  HADM_ID            ADMITTIME            DISCHTIME  \\\n",
       "0       1           2   163353  2138-07-17 19:04:00  2138-07-21 15:48:00   \n",
       "1       2           3   145834  2101-10-20 19:08:00  2101-10-31 13:58:00   \n",
       "2       3           4   185777  2191-03-16 00:28:00  2191-03-23 18:41:00   \n",
       "3       4           5   178980  2103-02-02 04:31:00  2103-02-04 12:15:00   \n",
       "4       5           6   107064  2175-05-30 07:15:00  2175-06-15 16:00:00   \n",
       "\n",
       "  DEATHTIME ADMISSION_TYPE         ADMISSION_LOCATION  \\\n",
       "0       NaN        NEWBORN  PHYS REFERRAL/NORMAL DELI   \n",
       "1       NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "2       NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "3       NaN        NEWBORN  PHYS REFERRAL/NORMAL DELI   \n",
       "4       NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "\n",
       "          DISCHARGE_LOCATION INSURANCE LANGUAGE           RELIGION  \\\n",
       "0                       HOME   Private      NaN      NOT SPECIFIED   \n",
       "1                        SNF  Medicare      NaN           CATHOLIC   \n",
       "2  HOME WITH HOME IV PROVIDR   Private      NaN  PROTESTANT QUAKER   \n",
       "3                       HOME   Private      NaN           BUDDHIST   \n",
       "4           HOME HEALTH CARE  Medicare     ENGL      NOT SPECIFIED   \n",
       "\n",
       "  MARITAL_STATUS ETHNICITY            EDREGTIME            EDOUTTIME  \\\n",
       "0            NaN     ASIAN                  NaN                  NaN   \n",
       "1        MARRIED     WHITE  2101-10-20 17:09:00  2101-10-20 19:24:00   \n",
       "2         SINGLE     WHITE  2191-03-15 13:10:00  2191-03-16 01:10:00   \n",
       "3            NaN     ASIAN                  NaN                  NaN   \n",
       "4        MARRIED     WHITE                  NaN                  NaN   \n",
       "\n",
       "                             DIAGNOSIS  HOSPITAL_EXPIRE_FLAG  \\\n",
       "0                              NEWBORN                     0   \n",
       "1                          HYPOTENSION                     0   \n",
       "2  FEVER,DEHYDRATION,FAILURE TO THRIVE                     0   \n",
       "3                              NEWBORN                     0   \n",
       "4            CHRONIC RENAL FAILURE/SDA                     0   \n",
       "\n",
       "   HAS_CHARTEVENTS_DATA  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Discharge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44005</td>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>2101-10-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2101-10-20**]     Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4788</td>\n",
       "      <td>4</td>\n",
       "      <td>185777.0</td>\n",
       "      <td>2191-03-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>20825</td>\n",
       "      <td>6</td>\n",
       "      <td>107064.0</td>\n",
       "      <td>2175-06-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date: [**2175-5-30**]        Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>57115</td>\n",
       "      <td>9</td>\n",
       "      <td>150750.0</td>\n",
       "      <td>2149-11-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Addendum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Name:  [**Known lastname 10050**], [**Known fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>20070</td>\n",
       "      <td>9</td>\n",
       "      <td>150750.0</td>\n",
       "      <td>2149-11-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2149-11-9**]       Dischar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "30    44005           3  145834.0  2101-10-31       NaN       NaN   \n",
       "102    4788           4  185777.0  2191-03-23       NaN       NaN   \n",
       "116   20825           6  107064.0  2175-06-15       NaN       NaN   \n",
       "158   57115           9  150750.0  2149-11-14       NaN       NaN   \n",
       "166   20070           9  150750.0  2149-11-13       NaN       NaN   \n",
       "\n",
       "              CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "30   Discharge summary      Report   NaN      NaN   \n",
       "102  Discharge summary      Report   NaN      NaN   \n",
       "116  Discharge summary      Report   NaN      NaN   \n",
       "158  Discharge summary    Addendum   NaN      NaN   \n",
       "166  Discharge summary      Report   NaN      NaN   \n",
       "\n",
       "                                                  TEXT  \n",
       "30   Admission Date:  [**2101-10-20**]     Discharg...  \n",
       "102  Admission Date:  [**2191-3-16**]     Discharge...  \n",
       "116  Admission Date: [**2175-5-30**]        Dischar...  \n",
       "158  Name:  [**Known lastname 10050**], [**Known fi...  \n",
       "166  Admission Date:  [**2149-11-9**]       Dischar...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discharge_summaries = noteevents[noteevents['CATEGORY'] == 'Discharge summary']\n",
    "discharge_summaries.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Notes for a Specific Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455212</th>\n",
       "      <td>97526</td>\n",
       "      <td>10006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2165-03-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Echo</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PATIENT/TEST INFORMATION:\\nIndication: Endocar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455213</th>\n",
       "      <td>842219</td>\n",
       "      <td>10006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2164-09-30</td>\n",
       "      <td>2164-09-30 23:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2164-9-30**] 11:15 PM\\n CHEST (PORTABLE AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455214</th>\n",
       "      <td>267561</td>\n",
       "      <td>10006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2165-08-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atrial fibrillation with a mean ventricular re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455215</th>\n",
       "      <td>859978</td>\n",
       "      <td>10006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2165-03-05</td>\n",
       "      <td>2165-03-05 09:29:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>PICC W/O PORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2165-3-5**] 9:29 AM\\n PICC LINE PLACMENT SC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455216</th>\n",
       "      <td>870643</td>\n",
       "      <td>10006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2165-06-02</td>\n",
       "      <td>2165-06-02 21:46:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CT HEAD W/O CONTRAST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2165-6-2**] 9:46 PM\\n CT HEAD W/O CONTRAST ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ROW_ID  SUBJECT_ID  HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "455212   97526       10006      NaN  2165-03-05                  NaN   \n",
       "455213  842219       10006      NaN  2164-09-30  2164-09-30 23:15:00   \n",
       "455214  267561       10006      NaN  2165-08-08                  NaN   \n",
       "455215  859978       10006      NaN  2165-03-05  2165-03-05 09:29:00   \n",
       "455216  870643       10006      NaN  2165-06-02  2165-06-02 21:46:00   \n",
       "\n",
       "       STORETIME   CATEGORY           DESCRIPTION  CGID  ISERROR  \\\n",
       "455212       NaN       Echo                Report   NaN      NaN   \n",
       "455213       NaN  Radiology   CHEST (PORTABLE AP)   NaN      NaN   \n",
       "455214       NaN        ECG                Report   NaN      NaN   \n",
       "455215       NaN  Radiology         PICC W/O PORT   NaN      NaN   \n",
       "455216       NaN  Radiology  CT HEAD W/O CONTRAST   NaN      NaN   \n",
       "\n",
       "                                                     TEXT  \n",
       "455212  PATIENT/TEST INFORMATION:\\nIndication: Endocar...  \n",
       "455213  [**2164-9-30**] 11:15 PM\\n CHEST (PORTABLE AP)...  \n",
       "455214  Atrial fibrillation with a mean ventricular re...  \n",
       "455215  [**2165-3-5**] 9:29 AM\\n PICC LINE PLACMENT SC...  \n",
       "455216  [**2165-6-2**] 9:46 PM\\n CT HEAD W/O CONTRAST ...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_id = 10006  # Replace with actual subject_id\n",
    "patient_notes = noteevents[noteevents['SUBJECT_ID'] == patient_id]\n",
    "patient_notes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Notes within a Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769224</td>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>2101-10-26</td>\n",
       "      <td>2101-10-26 06:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>769043</td>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>2101-10-24</td>\n",
       "      <td>2101-10-24 08:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-24**] 8:05 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>769130</td>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>2101-10-25</td>\n",
       "      <td>2101-10-25 06:32:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-25**] 6:32 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1260684</td>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>2101-10-21</td>\n",
       "      <td>2101-10-21 06:58:00</td>\n",
       "      <td>2101-10-21 07:15:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>21570.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Micu Progress Nursing Note:\\n\\nPatient arrived...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>769079</td>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>2101-10-24</td>\n",
       "      <td>2101-10-24 16:06:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-24**] 4:06 PM\\n CHEST (PORTABLE AP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "3   769224           3  145834.0  2101-10-26  2101-10-26 06:01:00   \n",
       "5   769043           3  145834.0  2101-10-24  2101-10-24 08:05:00   \n",
       "6   769130           3  145834.0  2101-10-25  2101-10-25 06:32:00   \n",
       "7  1260684           3  145834.0  2101-10-21  2101-10-21 06:58:00   \n",
       "8   769079           3  145834.0  2101-10-24  2101-10-24 16:06:00   \n",
       "\n",
       "             STORETIME       CATEGORY          DESCRIPTION     CGID  ISERROR  \\\n",
       "3                  NaN      Radiology  CHEST (PORTABLE AP)      NaN      NaN   \n",
       "5                  NaN      Radiology  CHEST (PORTABLE AP)      NaN      NaN   \n",
       "6                  NaN      Radiology  CHEST (PORTABLE AP)      NaN      NaN   \n",
       "7  2101-10-21 07:15:00  Nursing/other               Report  21570.0      NaN   \n",
       "8                  NaN      Radiology  CHEST (PORTABLE AP)      NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "3  [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...  \n",
       "5  [**2101-10-24**] 8:05 AM\\n CHEST (PORTABLE AP)...  \n",
       "6  [**2101-10-25**] 6:32 AM\\n CHEST (PORTABLE AP)...  \n",
       "7  Micu Progress Nursing Note:\\n\\nPatient arrived...  \n",
       "8  [**2101-10-24**] 4:06 PM\\n CHEST (PORTABLE AP)...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_range_notes = noteevents[(noteevents['CHARTDATE'] >= '2101-10-20') & (noteevents['CHARTDATE'] <= '2102-01-01')]\n",
    "date_range_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Nursing/other', 'ECG', 'Radiology', 'Echo', 'Discharge summary',\n",
       "       'Nursing', 'Physician ', 'Pharmacy', 'General', 'Respiratory ',\n",
       "       'Social Work', 'Case Management ', 'Nutrition', 'Rehab Services',\n",
       "       'Consult'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display unique categories in the NOTEEVENTS DataFrame\n",
    "noteevents['CATEGORY'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CATEGORY                                               TEXT\n",
      "0  Nursing/other  Neonatology Attending Triage Note\\n\\nBaby [**N...\n",
      "1  Nursing/other  Nursing Transfer note\\n\\n\\nPt admitted to NICU...\n",
      "2            ECG  Sinus rhythm\\nInferior/lateral ST-T changes ar...\n",
      "3      Radiology  [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...\n",
      "4            ECG  Sinus rhythm\\nA-V delay\\nNonspecific inferior ...\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of clinical notes\n",
    "print(noteevents[['CATEGORY', 'TEXT']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CLEANED_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Admission Date:  [**2101-10-20**]     Discharg...</td>\n",
       "      <td>Admission Date: Discharge Date: Date of Birth:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "      <td>Admission Date: Discharge Date: Date of Birth:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Admission Date: [**2175-5-30**]        Dischar...</td>\n",
       "      <td>Admission Date: Discharge Date: Date of Birth:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Name:  [**Known lastname 10050**], [**Known fi...</td>\n",
       "      <td>Name: , Unit No: Admission Date: Discharge Dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Admission Date:  [**2149-11-9**]       Dischar...</td>\n",
       "      <td>Admission Date: Discharge Date: Date of Birth:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TEXT  \\\n",
       "30   Admission Date:  [**2101-10-20**]     Discharg...   \n",
       "102  Admission Date:  [**2191-3-16**]     Discharge...   \n",
       "116  Admission Date: [**2175-5-30**]        Dischar...   \n",
       "158  Name:  [**Known lastname 10050**], [**Known fi...   \n",
       "166  Admission Date:  [**2149-11-9**]       Dischar...   \n",
       "\n",
       "                                          CLEANED_TEXT  \n",
       "30   Admission Date: Discharge Date: Date of Birth:...  \n",
       "102  Admission Date: Discharge Date: Date of Birth:...  \n",
       "116  Admission Date: Discharge Date: Date of Birth:...  \n",
       "158  Name: , Unit No: Admission Date: Discharge Dat...  \n",
       "166  Admission Date: Discharge Date: Date of Birth:...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove de-identification brackets and other unwanted characters\n",
    "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = text.strip()  # Remove leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'TEXT' column\n",
    "discharge_summaries['CLEANED_TEXT'] = discharge_summaries['TEXT'].apply(clean_text)\n",
    "\n",
    "# Display the first few cleaned discharge summaries\n",
    "discharge_summaries[['TEXT', 'CLEANED_TEXT']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords: ['blood' 'day' 'discharge' 'history' 'left' 'mg' 'patient' 'po' 'sig'\n",
      " 'tablet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extract keywords using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10, stop_words='english')\n",
    "X = vectorizer.fit_transform(discharge_summaries['CLEANED_TEXT'])\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          CLEANED_TEXT  SENTIMENT\n",
      "30   Admission Date: Discharge Date: Date of Birth:...   0.018190\n",
      "102  Admission Date: Discharge Date: Date of Birth:...   0.025120\n",
      "116  Admission Date: Discharge Date: Date of Birth:...   0.111348\n",
      "158  Name: , Unit No: Admission Date: Discharge Dat...   0.000000\n",
      "166  Admission Date: Discharge Date: Date of Birth:...  -0.021885\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate sentiment\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis\n",
    "discharge_summaries['SENTIMENT'] = discharge_summaries['CLEANED_TEXT'].apply(get_sentiment)\n",
    "\n",
    "# Display the first few rows with sentiment\n",
    "print(discharge_summaries[['CLEANED_TEXT', 'SENTIMENT']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a CSV file\n",
    "discharge_summaries.to_csv('cleaned_discharge_summaries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  TEXT  \\\n",
      "30   Admission Date:  [**2101-10-20**]     Discharg...   \n",
      "102  Admission Date:  [**2191-3-16**]     Discharge...   \n",
      "116  Admission Date: [**2175-5-30**]        Dischar...   \n",
      "158  Name:  [**Known lastname 10050**], [**Known fi...   \n",
      "166  Admission Date:  [**2149-11-9**]       Dischar...   \n",
      "\n",
      "                                          CLEANED_TEXT  \n",
      "30   Admission Date: Discharge Date: Date of Birth:...  \n",
      "102  Admission Date: Discharge Date: Date of Birth:...  \n",
      "116  Admission Date: Discharge Date: Date of Birth:...  \n",
      "158  Name: , Unit No: Admission Date: Discharge Dat...  \n",
      "166  Admission Date: Discharge Date: Date of Birth:...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_advanced(text):\n",
    "    # Remove de-identification brackets\n",
    "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "discharge_summaries['CLEANED_TEXT'] = discharge_summaries['TEXT'].apply(clean_text_advanced)\n",
    "print(discharge_summaries[['TEXT', 'CLEANED_TEXT']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords: ['blood' 'day' 'discharge' 'history' 'left' 'mg' 'patient' 'po' 'sig'\n",
      " 'tablet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extract keywords using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10, stop_words='english')\n",
    "X = vectorizer.fit_transform(discharge_summaries['CLEANED_TEXT'])\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          CLEANED_TEXT  SENTIMENT\n",
      "30   Admission Date: Discharge Date: Date of Birth:...   0.018190\n",
      "102  Admission Date: Discharge Date: Date of Birth:...   0.025120\n",
      "116  Admission Date: Discharge Date: Date of Birth:...   0.111348\n",
      "158  Name: , Unit No: Admission Date: Discharge Dat...   0.000000\n",
      "166  Admission Date: Discharge Date: Date of Birth:...  -0.021885\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate sentiment\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis\n",
    "discharge_summaries['SENTIMENT'] = discharge_summaries['CLEANED_TEXT'].apply(get_sentiment)\n",
    "print(discharge_summaries[['CLEANED_TEXT', 'SENTIMENT']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a CSV file\n",
    "discharge_summaries.to_csv('cleaned_discharge_summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Advanced Text Analysis\n",
    "1.1. Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(discharge_summaries['CLEANED_TEXT'])\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Display top words for each topic\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"Top words for topic #{index}\")\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Predictive Modeling\n",
    "2.1. Preparing Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a sample target variable 'readmitted'\n",
    "import numpy as np\n",
    "discharge_summaries['readmitted'] = np.random.randint(2, size=discharge_summaries.shape[0])\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(discharge_summaries['CLEANED_TEXT'])\n",
    "y = discharge_summaries['readmitted']  # Replace with actual target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Visualization\n",
    "3.1. Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a word cloud\n",
    "text = \" \".join(discharge_summaries['CLEANED_TEXT'])\n",
    "wordcloud = WordCloud(stopwords='english', background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CATEGORY                                               TEXT\n",
      "0  Nursing/other  Neonatology Attending Triage Note\\n\\nBaby [**N...\n",
      "1  Nursing/other  Nursing Transfer note\\n\\n\\nPt admitted to NICU...\n",
      "2            ECG  Sinus rhythm\\nInferior/lateral ST-T changes ar...\n",
      "3      Radiology  [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...\n",
      "4            ECG  Sinus rhythm\\nA-V delay\\nNonspecific inferior ...\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of clinical notes\n",
    "print(noteevents[['CATEGORY', 'TEXT']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for topic #0\n",
      "['status', 'care', 'history', 'date', 'admission', 'mg', 'blood', 'discharge', 'day', 'patient']\n",
      "\n",
      "\n",
      "Top words for topic #1\n",
      "['valve', 'history', 'aortic', 'cardiac', 'coronary', 'right', 'artery', 'patient', 'mg', 'left']\n",
      "\n",
      "\n",
      "Top words for topic #2\n",
      "['daily', 'disp', 'refills', 'discharge', 'po', 'mg', 'pt', 'sig', 'tablet', 'blood']\n",
      "\n",
      "\n",
      "Top words for topic #3\n",
      "['given', 'admission', 'pt', 'blood', 'discharge', 'history', 'ct', 'left', 'right', 'patient']\n",
      "\n",
      "\n",
      "Top words for topic #4\n",
      "['patient', '10', 'times', 'discharge', 'day', 'daily', 'sig', 'po', 'mg', 'tablet']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(discharge_summaries['CLEANED_TEXT'])\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Display top words for each topic\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"Top words for topic #{index}\")\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'readmitted'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'readmitted'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(discharge_summaries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLEANED_TEXT\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdischarge_summaries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreadmitted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Replace with actual target variable\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'readmitted'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming you have a 'readmitted' column indicating readmission status\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(discharge_summaries['CLEANED_TEXT'])\n",
    "y = discharge_summaries['readmitted']  # Replace with actual target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a word cloud\n",
    "text = \" \".join(discharge_summaries['CLEANED_TEXT'])\n",
    "wordcloud = WordCloud(stopwords='english', background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO', 'YES']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def isRegexMatching(regex, arr):\n",
    "    def match_pattern(pattern, string):\n",
    "        # This function will use Python's re module to match pattern with the string\n",
    "        try:\n",
    "            compiled_pattern = re.compile(pattern)\n",
    "            return bool(compiled_pattern.fullmatch(string))\n",
    "        except re.error:\n",
    "            return False\n",
    "    \n",
    "    # Parse the input regex to create a valid Python regex pattern\n",
    "    pattern = \"\"\n",
    "    i = 0\n",
    "    while i < len(regex):\n",
    "        if regex[i] == '.':\n",
    "            pattern += '.'\n",
    "        elif regex[i] == '*':\n",
    "            pattern += '*'\n",
    "        elif regex[i] == '(':\n",
    "            sub_pattern = \"\"\n",
    "            i += 1\n",
    "            while i < len(regex) and regex[i] != ')':\n",
    "                sub_pattern += regex[i]\n",
    "                i += 1\n",
    "            pattern += \"(\" + sub_pattern + \")\"\n",
    "        elif regex[i] == ')':\n",
    "            pattern += ')'\n",
    "        else:\n",
    "            pattern += regex[i]\n",
    "        i += 1\n",
    "    \n",
    "    results = []\n",
    "    for string in arr:\n",
    "        if match_pattern(pattern, string):\n",
    "            results.append(\"YES\")\n",
    "        else:\n",
    "            results.append(\"NO\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sample Input\n",
    "regex = \"ab(e.r)*e\"\n",
    "arr = [\"abbeere\", \"abefretre\"]\n",
    "\n",
    "# Call the function with the sample input\n",
    "result = isRegexMatching(regex, arr)\n",
    "print(result)  # Expected Output: [\"NO\", \"YES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RegexEngine() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ab)*c.d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mababcd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 66\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mRegexEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(engine\u001b[38;5;241m.\u001b[39mmatch(text))\n",
      "\u001b[1;31mTypeError\u001b[0m: RegexEngine() takes no arguments"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class RegexEngine:\n",
    "    def _init_(self, pattern):\n",
    "        self.pattern = pattern\n",
    "        self.parsed_pattern = self.parse_pattern(pattern)\n",
    "        \n",
    "    def parse_pattern(self, pattern):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(pattern):\n",
    "            if pattern[i] in \"abcdefghijklmnopqrstuvwxyz.\":\n",
    "                tokens.append(pattern[i])\n",
    "                i += 1\n",
    "            elif pattern[i] == '(':\n",
    "                j = i + 1\n",
    "                while pattern[j] != ')':\n",
    "                    j += 1\n",
    "                tokens.append(pattern[i:j+1])\n",
    "                i = j + 1\n",
    "            elif pattern[i] == '*':\n",
    "                tokens[-1] += '*'\n",
    "                i += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid character in pattern: {pattern[i]}\")\n",
    "        return tokens\n",
    "    \n",
    "    def match(self, text):\n",
    "        return self.match_helper(text, self.parsed_pattern)\n",
    "    \n",
    "    def match_helper(self, text, tokens):\n",
    "        if not tokens:\n",
    "            return text == \"\"\n",
    "        \n",
    "        first, rest = tokens[0], tokens[1:]\n",
    "        \n",
    "        if first.endswith('*'):\n",
    "            token = first[:-1]\n",
    "            return self.match_star(token, text, rest)\n",
    "        else:\n",
    "            if text and self.match_token(first, text[0]):\n",
    "                return self.match_helper(text[1:], rest)\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    def match_star(self, token, text, rest):\n",
    "        for i in range(len(text) + 1):\n",
    "            if self.match_helper(text[i:], rest):\n",
    "                return True\n",
    "            if i < len(text) and not self.match_token(token, text[i]):\n",
    "                break\n",
    "        return False\n",
    "    \n",
    "    def match_token(self, token, char):\n",
    "        if len(token) == 1:\n",
    "            return token == char or (token == '.' and char.islower())\n",
    "        elif token.startswith('(') and token.endswith(')'):\n",
    "            subpattern = token[1:-1]\n",
    "            return self.match_helper(char, self.parse_pattern(subpattern))\n",
    "        return False\n",
    "\n",
    "# Usage example:\n",
    "pattern = \"(ab)*c.d\"\n",
    "text = \"ababcd\"\n",
    "\n",
    "engine = RegexEngine(pattern)\n",
    "print(engine.match(text))  # True or False dependingonthematch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset splits\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "import feature_extraction as fx\n",
    "\n",
    "# Load dataset splits\n",
    "mimic = datasets.MIMIC_Dataset()  # Initializes a MIMIC dataset object\n",
    "mimic.load_preprocessed()         # Loads preprocessed MIMIC-III data\n",
    "mimic.split()                     # Splits the dataset into train/test sets\n",
    "\n",
    "# Load embeddings\n",
    "embedding = fx.W2V(mimic.name)    # Initializes Word2Vec embeddings\n",
    "embedding.transform(mimic)        # Transforms the MIMIC dataset using embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Microsoft Corporation.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Licensed under the MIT License.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# code was adapted for our datasets\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "# code was adapted for our datasets\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as DataLoader_geo\n",
    "\n",
    "from graphormer.collator import collator\n",
    "from graphormer.utils.evaluators import FeatAccuracy\n",
    "from graphormer.wrapper import MyTadpoleDataset, MyMIMICDataset\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name='abaaba', cross_val_split=None, drop_val_patients=None, fold=None, task=None, num_graphs=43,\n",
    "                edge_vars='full_edge_full_node', gcn=False, mlp=False, rotation=0, pad_mode='original', mask_ratio=0.15, block_size=6, sim_graph=True,\n",
    "                mask_all=False, k=5):\n",
    "    global dataset\n",
    "    global cross_val_datasets\n",
    "    if dataset is not None and cross_val_split is None:\n",
    "        return dataset\n",
    "    elif cross_val_split in cross_val_datasets:  # need to load new dataset for this split\n",
    "        return cross_val_datasets[cross_val_split]\n",
    "\n",
    "    elif dataset_name == 'tadpole':\n",
    "        mask = True if dataset_name == 'tadpole' else False\n",
    "        dataset = {\n",
    "            'num_class': None,  # need to treat binary and regression features separately\n",
    "            'loss_fn': F.cross_entropy,\n",
    "            'metric': 'macro_acc_avg',\n",
    "            'metric_mode': 'max',\n",
    "            'evaluator': FeatAccuracy(),\n",
    "            'train_dataset': MyTadpoleDataset(root='data/tadpole', mask=mask, name='tadpole', raw_file_name='tadpole_numerical.csv', offset=96,\n",
    "                                              bin_split_idx=5, split='train', drop_val_patients=drop_val_patients, cross_val_split=cross_val_split,\n",
    "                                              fold=fold, sim_graph=sim_graph, mask_all=mask_all, mask_ratio=mask_ratio),\n",
    "            'valid_dataset': MyTadpoleDataset(root='data/tadpole', mask=mask, name='tadpole', raw_file_name='tadpole_numerical.csv', offset=96,\n",
    "                                              bin_split_idx=5, split='val', drop_val_patients=drop_val_patients, cross_val_split=cross_val_split,\n",
    "                                              fold=fold, sim_graph=sim_graph, mask_all=mask_all, mask_ratio=mask_ratio),\n",
    "            'update_mask': None,\n",
    "            'max_node': 565,\n",
    "        }\n",
    "        if cross_val_split is not None:\n",
    "            cross_val_datasets[cross_val_split] = dataset\n",
    "\n",
    "    elif dataset_name == 'tadpole_class':\n",
    "        mask = True if dataset_name == 'tadpole' else False\n",
    "        dataset = {\n",
    "            'num_class': None,  # need to treat binary and regression features separately\n",
    "            'loss_fn': F.cross_entropy,\n",
    "            'metric': 'macro_acc_avg',\n",
    "            'metric_mode': 'max',\n",
    "            'evaluator': FeatAccuracy(),\n",
    "            'train_dataset': MyTadpoleDataset(root='data/tadpole', mask=mask, name='tadpole', raw_file_name='tadpole_numerical.csv', offset=96,\n",
    "                                              bin_split_idx=5, split='train', drop_val_patients=drop_val_patients, cross_val_split=cross_val_split,\n",
    "                                              fold=fold, sim_graph=sim_graph, k=k),\n",
    "            'valid_dataset': MyTadpoleDataset(root='data/tadpole', mask=mask, name='tadpole', raw_file_name='tadpole_numerical.csv', offset=96,\n",
    "                                              bin_split_idx=5, split='val', drop_val_patients=drop_val_patients, cross_val_split=cross_val_split,\n",
    "                                              fold=fold, sim_graph=sim_graph, k=k),\n",
    "            'update_mask': None,\n",
    "            'max_node': 565,\n",
    "        }\n",
    "        if cross_val_split is not None:\n",
    "            cross_val_datasets[cross_val_split] = dataset\n",
    "\n",
    "    elif dataset_name == 'mimic':\n",
    "        if task.startswith('pre'):\n",
    "            dataset = {\n",
    "                'num_class': None,  # need to treat binary and regression features separately\n",
    "                'loss_fn': F.cross_entropy,\n",
    "                'metric': 'macro_acc_avg',\n",
    "                'metric_mode': 'max',\n",
    "                'evaluator': FeatAccuracy(),\n",
    "                'train_dataset': MyMIMICDataset(root=f'data/mimic-iii-0', drop_val_patients=True, use_treatment_input=True, task=task, split='train',\n",
    "                                                edge_vars=edge_vars, num_graphs=num_graphs, gcn=gcn, mlp=mlp, rotation=rotation, pad_mode=pad_mode,\n",
    "                                                mask_ratio=mask_ratio, block_size=block_size),\n",
    "                'valid_dataset': MyMIMICDataset(root=f'data/mimic-iii-0', drop_val_patients=True, use_treatment_input=True, task=task, split='val',\n",
    "                                                edge_vars=edge_vars, num_graphs=num_graphs, gcn=gcn, mlp=mlp, rotation=rotation, pad_mode=pad_mode,\n",
    "                                                mask_ratio=mask_ratio, block_size=block_size),\n",
    "                'test_dataset': MyMIMICDataset(root=f'data/mimic-iii-0', drop_val_patients=True, use_treatment_input=True, task=task, split='test',\n",
    "                                               edge_vars=edge_vars, num_graphs=num_graphs, gcn=gcn, mlp=mlp, rotation=rotation, pad_mode=pad_mode,\n",
    "                                               mask_ratio=mask_ratio, block_size=block_size),\n",
    "                'predict_dataset': MyMIMICDataset(root=f'data/mimic-iii-0', drop_val_patients=True, use_treatment_input=True, task=task, split='val',\n",
    "                                                  edge_vars=edge_vars, num_graphs=num_graphs, gcn=gcn, mlp=mlp, rotation=rotation, predict=True,\n",
    "                                                  pad_mode=pad_mode, mask_ratio=mask_ratio, block_size=block_size),\n",
    "                'update_mask': None,\n",
    "                'max_node': 550,\n",
    "            }\n",
    "        else:\n",
    "            dataset = {\n",
    "                'num_class': None,  # need to treat binary and regression features separately\n",
    "                'loss_fn': F.cross_entropy,\n",
    "                'metric': 'macro_acc_avg',\n",
    "                'metric_mode': 'max',\n",
    "                'evaluator': FeatAccuracy(),\n",
    "                'train_dataset': MyMIMICDataset(root=f'data/mimic-iii-0', drop_val_patients=True, use_treatment_input=True, task=task, split='train',\n",
    "                                                edge_vars=edge_vars, num_graphs=num_graphs, gcn=gcn, mlp=mlp, rotation=rotation, pad_mode=pad_mode,\n",
    "                                                k=k),\n",
    "                'valid_dataset': MyMIMICDataset(root=f'data/mimic-iii-0', drop_val_patients=True, use_treatment_input=True, task=task, split='val',\n",
    "                                                edge_vars=edge_vars, num_graphs=num_graphs, gcn=gcn, mlp=mlp, rotation=rotation, pad_mode=pad_mode,\n",
    "                                                k=k),\n",
    "                'test_dataset': MyMIMICDataset(root=f'data/mimic-iii-0', drop_val_patients=True, use_treatment_input=True, task=task, split='test',\n",
    "                                               edge_vars=edge_vars, num_graphs=num_graphs, gcn=gcn, mlp=mlp, rotation=rotation, pad_mode=pad_mode,\n",
    "                                               k=k),\n",
    "                'update_mask': None,\n",
    "                'max_node': 550,\n",
    "            }\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    print(f' > {dataset_name} loaded!')\n",
    "    print(dataset)\n",
    "    print(f' > dataset info ends')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class GraphDataModule(LightningDataModule):\n",
    "    name = \"OGB-GRAPH\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_name: str = 'ogbg-molpcba',\n",
    "            cross_val_split=None,\n",
    "            drop_val_patients=True,\n",
    "            fold=None,\n",
    "            task=None,\n",
    "            num_graphs=None,\n",
    "            rotation=0,\n",
    "            edge_vars='full_edge_full_node',\n",
    "            num_workers: int = 0,\n",
    "            batch_size: int = 256,\n",
    "            seed: int = 42,\n",
    "            multi_hop_max_dist: int = 5,\n",
    "            spatial_pos_max: int = 1024,\n",
    "            gcn=False,\n",
    "            mlp=False,\n",
    "            pad_mode='original',\n",
    "            mask_ratio=0.15,\n",
    "            block_size=6,\n",
    "            sim_graph=True,\n",
    "            mask_all=False,\n",
    "            k=5,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = get_dataset(self.dataset_name, cross_val_split=cross_val_split, drop_val_patients=drop_val_patients, fold=fold, task=task,\n",
    "                                   num_graphs=num_graphs, edge_vars=edge_vars, gcn=gcn, mlp=mlp, rotation=rotation, pad_mode=pad_mode,\n",
    "                                   mask_ratio=mask_ratio, block_size=block_size, mask_all=mask_all, sim_graph=sim_graph, k=k)\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_train = ...\n",
    "        self.dataset_val = ...\n",
    "        self.multi_hop_max_dist = multi_hop_max_dist\n",
    "        self.spatial_pos_max = spatial_pos_max\n",
    "        self.gcn = gcn\n",
    "        self.mlp = mlp\n",
    "        self.task = task\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        self.dataset_train = self.dataset['train_dataset']\n",
    "        self.dataset_val = self.dataset['valid_dataset']\n",
    "        if 'test_dataset' in self.dataset.keys():\n",
    "            self.dataset_test = self.dataset['test_dataset']\n",
    "        if 'predict_dataset' in self.dataset.keys():\n",
    "            self.dataset_predict = self.dataset['predict_dataset']\n",
    "        self.collator = collator\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.gcn or self.mlp:\n",
    "            if self.task == \"pre_mask\":\n",
    "                loader = DataLoader(\n",
    "                    self.dataset_train,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=self.num_workers,\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=partial(self.collator, dataset=self.dataset_name, max_node=get_dataset(self.dataset_name)[\n",
    "                        'max_node'], multi_hop_max_dist=self.multi_hop_max_dist, spatial_pos_max=self.spatial_pos_max, gcn=self.gcn or self.mlp,\n",
    "                                       pad_mode=self.pad_mode),\n",
    "                )\n",
    "            else:\n",
    "                loader = DataLoader_geo(\n",
    "                    dataset=self.dataset_train,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=self.num_workers,\n",
    "                )\n",
    "        else:\n",
    "            loader = DataLoader(\n",
    "                self.dataset_train,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=True,\n",
    "                collate_fn=partial(self.collator, dataset=self.dataset_name, max_node=get_dataset(self.dataset_name)[\n",
    "                    'max_node'], multi_hop_max_dist=self.multi_hop_max_dist, spatial_pos_max=self.spatial_pos_max, gcn=self.gcn,\n",
    "                                   pad_mode=self.pad_mode),\n",
    "            )\n",
    "        print('len(train_dataloader)', len(loader))\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.gcn or self.mlp:\n",
    "            if self.task == \"pre_mask\":\n",
    "                loader = DataLoader(\n",
    "                    self.dataset_val,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=self.num_workers,\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=partial(self.collator, dataset=self.dataset_name, max_node=get_dataset(self.dataset_name)[\n",
    "                        'max_node'], multi_hop_max_dist=self.multi_hop_max_dist, spatial_pos_max=self.spatial_pos_max, gcn=self.gcn or self.mlp,\n",
    "                                       pad_mode=self.pad_mode),\n",
    "                )\n",
    "            else:\n",
    "                loader = DataLoader_geo(\n",
    "                    dataset=self.dataset_val,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=self.num_workers,\n",
    "                )\n",
    "        else:\n",
    "            loader = DataLoader(\n",
    "                self.dataset_val,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=False,\n",
    "                collate_fn=partial(self.collator, dataset=self.dataset_name, max_node=get_dataset(self.dataset_name)[\n",
    "                    'max_node'], multi_hop_max_dist=self.multi_hop_max_dist, spatial_pos_max=self.spatial_pos_max, gcn=self.gcn,\n",
    "                                   pad_mode=self.pad_mode),\n",
    "            )\n",
    "        print('len(val_dataloader)', len(loader))\n",
    "        return loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.gcn or self.mlp:\n",
    "            if self.task == \"pre_mask\":\n",
    "                loader = DataLoader(\n",
    "                    self.dataset_test,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=self.num_workers,\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=partial(self.collator, dataset=self.dataset_name, max_node=get_dataset(self.dataset_name)[\n",
    "                        'max_node'], multi_hop_max_dist=self.multi_hop_max_dist, spatial_pos_max=self.spatial_pos_max, gcn=self.gcn or self.mlp,\n",
    "                                       pad_mode=self.pad_mode),\n",
    "                )\n",
    "            else:\n",
    "                loader = DataLoader_geo(\n",
    "                    dataset=self.dataset_test,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=self.num_workers,\n",
    "                )\n",
    "        else:\n",
    "            loader = DataLoader(\n",
    "                self.dataset_test,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=False,\n",
    "                collate_fn=partial(self.collator, dataset=self.dataset_name, max_node=get_dataset(self.dataset_name)[\n",
    "                    'max_node'], multi_hop_max_dist=self.multi_hop_max_dist, spatial_pos_max=self.spatial_pos_max, gcn=self.gcn,\n",
    "                                   pad_mode=self.pad_mode),\n",
    "            )\n",
    "        return loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        loader = DataLoader(\n",
    "            self.dataset_predict,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=False,\n",
    "            collate_fn=partial(self.collator, dataset=self.dataset_name, max_node=get_dataset(self.dataset_name)[\n",
    "                'max_node'], multi_hop_max_dist=self.multi_hop_max_dist, spatial_pos_max=self.spatial_pos_max, pad_mode=self.pad_mode),\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
